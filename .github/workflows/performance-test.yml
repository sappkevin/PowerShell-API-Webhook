name: Performance Testing

on:
  workflow_dispatch:
    inputs:
      concurrentUsers:
        description: 'Number of concurrent users to simulate'
        required: true
        default: '50'
      testDuration:
        description: 'Test duration in minutes'
        required: true
        default: '5'
      apiUrl:
        description: 'API URL to test (defaults to local service)'
        required: false
        default: 'http://localhost:80'
  pull_request:
    types: [labeled]
  workflow_call:
    inputs:
      concurrentUsers:
        type: string
        required: false
        default: '50'
      testDuration:
        type: string
        required: false
        default: '5'
      apiUrl:
        type: string
        required: false
        default: 'http://localhost:80'

jobs:
  performance-test-linux:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
      
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiohttp rich
      
    - name: Setup .NET 8.0 (for API service)
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '8.0.x'
      
    - name: Build and Start Docker Container for Testing
      run: |
          # Build the Docker image 
          docker build -t webhookshell:test -f Dockerfile.linux .
          
          # Run the container, exposing port 80 to host port 80
          docker run -d --name test-webhookshell -p 80:80 webhookshell:test
          
          # Wait for container to initialize
          echo "Waiting for container to start..."
          sleep 30
          
          # Verify container is running and health endpoint is accessible
          docker ps
          docker logs test-webhookshell
          curl -v http://localhost:80/health || echo "Health check failed"
      
    - name: Copy load testing script
      run: |
        mkdir -p performance_tests
        chmod +rwx performance_tests
        cat > performance_tests/webhook_load_tester.py << 'EOL'
        #!/usr/bin/env python3
        #!/usr/bin/env python3
        import argparse
        import asyncio
        import aiohttp
        import json
        import time
        import statistics
        from datetime import datetime
        import os
        from rich.console import Console
        from rich.table import Table
        from rich.progress import Progress, TaskID

        console = Console()

        def main():
            parser = argparse.ArgumentParser(description="WebhookShell API Load Tester")
            parser.add_argument("--api-url", default="http://localhost:80", help="API URL to test")
            parser.add_argument("--api-key", default="24ffc5be-7dd8-479f-898e-27169bf23e7f", help="API key for authentication")
            parser.add_argument("--concurrent-users", type=int, default=50, help="Number of concurrent users")
            parser.add_argument("--duration", type=int, default=5, help="Test duration in minutes")
            
            args = parser.parse_args()
            
            # Ensure reports directory exists
            os.makedirs("reports", exist_ok=True)
            
            tester = WebhookLoadTester(
                api_url=args.api_url,
                api_key=args.api_key,
                concurrent_users=args.concurrent_users,
                duration_minutes=args.duration
            )
            
            asyncio.run(tester.run_test())

        class WebhookLoadTester:
            def __init__(self, api_url, api_key, concurrent_users, duration_minutes):
                self.api_url = api_url
                self.api_key = api_key
                self.concurrent_users = concurrent_users
                self.duration_seconds = duration_minutes * 60
                self.results = {
                    "GET API": {"success": 0, "fail": 0, "latencies": []},
                    "POST API": {"success": 0, "fail": 0, "latencies": []},
                    "Background Job": {"success": 0, "fail": 0, "latencies": []}
                }
            
            async def run_test(self):
                console.print(f"[bold green]Starting load test against {self.api_url}[/bold green]")
                console.print(f"Concurrent users: {self.concurrent_users}")
                console.print(f"Test duration: {self.duration_seconds // 60} minutes")
                
                # Create a shared session for all requests
                async with aiohttp.ClientSession() as session:
                    # Set up progress tracking
                    with Progress() as progress:
                        get_task = progress.add_task("[cyan]GET API Requests...", total=self.duration_seconds)
                        post_task = progress.add_task("[magenta]POST API Requests...", total=self.duration_seconds)
                        bg_task = progress.add_task("[yellow]Background Job Requests...", total=self.duration_seconds)
                        
                        # Start the test scenarios
                        await asyncio.gather(
                            self._run_get_scenario(session, progress, get_task),
                            self._run_post_scenario(session, progress, post_task),
                            self._run_background_job_scenario(session, progress, bg_task)
                        )
                
                # Generate report
                self._generate_report()

            async def _run_get_scenario(self, session, progress, task_id):
                start_time = time.time()
                payload = {
                    "script": "health-check-script.ps1",
                    "key": self.api_key
                }
                
                # Build query parameters
                query_params = "&".join([f"{k}={v}" for k, v in payload.items()])
                endpoint = f"{self.api_url}/webhook/v1?{query_params}"
                
                while time.time() - start_time < self.duration_seconds:
                    # Create a batch of concurrent requests
                    tasks = []
                    for _ in range(self.concurrent_users // 3):  # Divide users among 3 scenarios
                        tasks.append(self._send_get_request(session, endpoint))
                    
                    # Wait for the batch to complete
                    await asyncio.gather(*tasks)
                    
                    # Update progress
                    elapsed = min(time.time() - start_time, self.duration_seconds)
                    progress.update(task_id, completed=elapsed)
                    
                    # Rate limiting: 10 requests per second (per scenario)
                    await asyncio.sleep(0.1)

            async def _run_post_scenario(self, session, progress, task_id):
                start_time = time.time()
                payload = {
                    "script": "health-check-script.ps1",
                    "key": self.api_key
                }
                
                endpoint = f"{self.api_url}/webhook/v1"
                
                while time.time() - start_time < self.duration_seconds:
                    # Create a batch of concurrent requests
                    tasks = []
                    for _ in range(self.concurrent_users // 3):  # Divide users among 3 scenarios
                        tasks.append(self._send_post_request(session, endpoint, payload))
                    
                    # Wait for the batch to complete
                    await asyncio.gather(*tasks)
                    
                    # Update progress
                    elapsed = min(time.time() - start_time, self.duration_seconds)
                    progress.update(task_id, completed=elapsed)
                    
                    # Rate limiting: 10 requests per second (per scenario)
                    await asyncio.sleep(0.1)

            async def _run_background_job_scenario(self, session, progress, task_id):
                start_time = time.time()
                payload = {
                    "script": "health-check-script.ps1",
                    "key": self.api_key
                }
                
                endpoint = f"{self.api_url}/jobs/v1/enqueue"
                
                while time.time() - start_time < self.duration_seconds:
                    # Create a batch of concurrent requests
                    tasks = []
                    for _ in range(self.concurrent_users // 3):  # Divide users among 3 scenarios
                        tasks.append(self._send_post_request(session, endpoint, payload, "Background Job"))
                    
                    # Wait for the batch to complete
                    await asyncio.gather(*tasks)
                    
                    # Update progress
                    elapsed = min(time.time() - start_time, self.duration_seconds)
                    progress.update(task_id, completed=elapsed)
                    
                    # Rate limiting: 15 requests per second (per scenario)
                    await asyncio.sleep(0.067)  # ~15 rps

            async def _send_get_request(self, session, url):
                start_time = time.time()
                try:
                    async with session.get(url, headers={"Accept": "application/json"}) as response:
                        await response.read()  # Read the full response
                        latency = time.time() - start_time
                        self.results["GET API"]["latencies"].append(latency)
                        
                        if response.status < 400:
                            self.results["GET API"]["success"] += 1
                        else:
                            self.results["GET API"]["fail"] += 1
                except Exception as e:
                    console.print(f"[red]GET request error: {e}[/red]")
                    self.results["GET API"]["fail"] += 1

            async def _send_post_request(self, session, url, payload, scenario="POST API"):
                start_time = time.time()
                try:
                    async with session.post(
                        url, 
                        json=payload,
                        headers={
                            "Content-Type": "application/json",
                            "Accept": "application/json"
                        }
                    ) as response:
                        await response.read()  # Read the full response
                        latency = time.time() - start_time
                        self.results[scenario]["latencies"].append(latency)
                        
                        if response.status < 400:
                            self.results[scenario]["success"] += 1
                        else:
                            self.results[scenario]["fail"] += 1
                except Exception as e:
                    console.print(f"[red]{scenario} request error: {e}[/red]")
                    self.results[scenario]["fail"] += 1

            def _generate_report(self):
                # Create reports directory
                os.makedirs("reports", exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Create console report
                table = Table(title="WebhookShell API Load Test Results")
                table.add_column("Scenario", style="cyan")
                table.add_column("Success", style="green")
                table.add_column("Fail", style="red")
                table.add_column("RPS", style="yellow")
                table.add_column("Min (ms)", style="blue")
                table.add_column("Max (ms)", style="blue")
                table.add_column("Avg (ms)", style="blue")
                table.add_column("p95 (ms)", style="magenta")
                
                for scenario, data in self.results.items():
                    if data["latencies"]:
                        latencies_ms = [l * 1000 for l in data["latencies"]]
                        rps = (data["success"] + data["fail"]) / self.duration_seconds
                        
                        min_latency = min(latencies_ms)
                        max_latency = max(latencies_ms)
                        avg_latency = statistics.mean(latencies_ms)
                        p95_latency = sorted(latencies_ms)[int(len(latencies_ms) * 0.95)]
                        
                        table.add_row(
                            scenario,
                            str(data["success"]),
                            str(data["fail"]),
                            f"{rps:.2f}",
                            f"{min_latency:.2f}",
                            f"{max_latency:.2f}",
                            f"{avg_latency:.2f}",
                            f"{p95_latency:.2f}"
                        )
                    else:
                        table.add_row(scenario, "0", "0", "0", "N/A", "N/A", "N/A", "N/A")
                
                console.print(table)
                
                # Save results to file
                report_file = f"reports/load_test_results_{timestamp}.json"
                with open(report_file, "w") as f:
                    json.dump({
                        "test_info": {
                            "api_url": self.api_url,
                            "concurrent_users": self.concurrent_users,
                            "duration_seconds": self.duration_seconds,
                            "timestamp": timestamp
                        },
                        "results": {
                            scenario: {
                                "success": data["success"],
                                "fail": data["fail"],
                                "latencies_stats": {
                                    "min_ms": min(data["latencies"]) * 1000 if data["latencies"] else 0,
                                    "max_ms": max(data["latencies"]) * 1000 if data["latencies"] else 0,
                                    "avg_ms": statistics.mean(data["latencies"]) * 1000 if data["latencies"] else 0,
                                    "p95_ms": sorted([l * 1000 for l in data["latencies"]])[int(len(data["latencies"]) * 0.95)] if data["latencies"] else 0
                                }
                            } for scenario, data in self.results.items()
                        }
                    }, f, indent=2)
                
                # Generate Markdown report summary for GitHub
                summary_file = "reports/summary.md"
                with open(summary_file, "w") as f:
                    f.write("## Performance Test Summary\n\n")
                    f.write(f"- Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"- API URL: {self.api_url}\n")
                    f.write(f"- Concurrent Users: {self.concurrent_users}\n")
                    f.write(f"- Test Duration: {self.duration_seconds // 60} minutes\n\n")
                    
                    f.write("### Results Summary\n\n")
                    f.write("| Scenario | Success | Fail | RPS | Avg (ms) | p95 (ms) |\n")
                    f.write("|----------|---------|------|-----|----------|----------|\n")
                    
                    for scenario, data in self.results.items():
                        if data["latencies"]:
                            latencies_ms = [l * 1000 for l in data["latencies"]]
                            rps = (data["success"] + data["fail"]) / self.duration_seconds
                            avg_latency = statistics.mean(latencies_ms)
                            p95_latency = sorted(latencies_ms)[int(len(latencies_ms) * 0.95)]
                            
                            f.write(f"| {scenario}  |     {data['success']}    |     {data['fail']}     |     {rps:.2f}     |     {avg_latency:.2f}     |     {p95_latency:.2f}     |\n")
                        else:
                            f.write(f"| {scenario}  |     0     |     0     |     0     |     N/A     |     N/A    |\n")
                
                console.print(f"[bold green]Load test completed. Results saved to {report_file}[/bold green]")

        if __name__ == "__main__":
            main()
        EOL
            
    - name: Run Performance Tests
      run: |
            mkdir -p performance_tests/reports
            chmod +rwx performance_tests/reports
            chmod +rwx performance_tests/webhook_load_tester.py
            cd performance_tests
            python webhook_load_tester.py --api-url ${{ github.event.inputs.apiUrl || 'http://localhost:80' }} --concurrent-users ${{ github.event.inputs.concurrentUsers || '50' }} --duration ${{ github.event.inputs.testDuration || '5' }}
            
    - name: Upload Test Reports
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-reports
        path: performance_tests/reports/
        if-no-files-found: error  # Change to error to catch issues

    - name: Create PR Comment with Test Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        github-token: ${{secrets.GITHUB_TOKEN}}
        script: |
            const fs = require('fs');
            const summaryContent = fs.readFileSync('performance_tests/reports/summary.md', 'utf8');
            
            github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summaryContent
            });
            
    - name: Display Test Summary for Linux Build
      run: |
            cat performance_tests/reports/summary.md

  performance-test-windows:
    runs-on: windows-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
      
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiohttp rich
      
    - name: Setup .NET 8.0 (for API service)
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '8.0.x'
      
    - name: Build and Start Docker Container for Testing
      run: |
          # Build the Docker image 
          docker build -t webhookshell:test -f Dockerfile.windows .
          
          # Run the container, exposing port 80 to host port 80
          docker run -d --name test-webhookshell -p 80:80 webhookshell:test
          
          # Wait for container to initialize
          echo "Waiting for container to start..."
          sleep 30
          
          # Verify container is running and health endpoint is accessible
          docker ps
          docker logs test-webhookshell
          curl -v http://localhost:80/health || echo "Health check failed"
      
    - name: Copy load testing script
      run: |
        mkdir -p performance_tests
        chmod +rwx performance_tests
        cat > performance_tests/webhook_load_tester.py << 'EOL'
        #!/usr/bin/env python3
        #!/usr/bin/env python3
        import argparse
        import asyncio
        import aiohttp
        import json
        import time
        import statistics
        from datetime import datetime
        import os
        from rich.console import Console
        from rich.table import Table
        from rich.progress import Progress, TaskID

        console = Console()

        def main():
            parser = argparse.ArgumentParser(description="WebhookShell API Load Tester")
            parser.add_argument("--api-url", default="http://localhost:80", help="API URL to test")
            parser.add_argument("--api-key", default="24ffc5be-7dd8-479f-898e-27169bf23e7f", help="API key for authentication")
            parser.add_argument("--concurrent-users", type=int, default=50, help="Number of concurrent users")
            parser.add_argument("--duration", type=int, default=5, help="Test duration in minutes")
            
            args = parser.parse_args()
            
            # Ensure reports directory exists
            os.makedirs("reports", exist_ok=True)
            
            tester = WebhookLoadTester(
                api_url=args.api_url,
                api_key=args.api_key,
                concurrent_users=args.concurrent_users,
                duration_minutes=args.duration
            )
            
            asyncio.run(tester.run_test())

        class WebhookLoadTester:
            def __init__(self, api_url, api_key, concurrent_users, duration_minutes):
                self.api_url = api_url
                self.api_key = api_key
                self.concurrent_users = concurrent_users
                self.duration_seconds = duration_minutes * 60
                self.results = {
                    "GET API": {"success": 0, "fail": 0, "latencies": []},
                    "POST API": {"success": 0, "fail": 0, "latencies": []},
                    "Background Job": {"success": 0, "fail": 0, "latencies": []}
                }
            
            async def run_test(self):
                console.print(f"[bold green]Starting load test against {self.api_url}[/bold green]")
                console.print(f"Concurrent users: {self.concurrent_users}")
                console.print(f"Test duration: {self.duration_seconds // 60} minutes")
                
                # Create a shared session for all requests
                async with aiohttp.ClientSession() as session:
                    # Set up progress tracking
                    with Progress() as progress:
                        get_task = progress.add_task("[cyan]GET API Requests...", total=self.duration_seconds)
                        post_task = progress.add_task("[magenta]POST API Requests...", total=self.duration_seconds)
                        bg_task = progress.add_task("[yellow]Background Job Requests...", total=self.duration_seconds)
                        
                        # Start the test scenarios
                        await asyncio.gather(
                            self._run_get_scenario(session, progress, get_task),
                            self._run_post_scenario(session, progress, post_task),
                            self._run_background_job_scenario(session, progress, bg_task)
                        )
                
                # Generate report
                self._generate_report()

            async def _run_get_scenario(self, session, progress, task_id):
                start_time = time.time()
                payload = {
                    "script": "health-check-script.ps1",
                    "key": self.api_key
                }
                
                # Build query parameters
                query_params = "&".join([f"{k}={v}" for k, v in payload.items()])
                endpoint = f"{self.api_url}/webhook/v1?{query_params}"
                
                while time.time() - start_time < self.duration_seconds:
                    # Create a batch of concurrent requests
                    tasks = []
                    for _ in range(self.concurrent_users // 3):  # Divide users among 3 scenarios
                        tasks.append(self._send_get_request(session, endpoint))
                    
                    # Wait for the batch to complete
                    await asyncio.gather(*tasks)
                    
                    # Update progress
                    elapsed = min(time.time() - start_time, self.duration_seconds)
                    progress.update(task_id, completed=elapsed)
                    
                    # Rate limiting: 10 requests per second (per scenario)
                    await asyncio.sleep(0.1)

            async def _run_post_scenario(self, session, progress, task_id):
                start_time = time.time()
                payload = {
                    "script": "health-check-script.ps1",
                    "key": self.api_key
                }
                
                endpoint = f"{self.api_url}/webhook/v1"
                
                while time.time() - start_time < self.duration_seconds:
                    # Create a batch of concurrent requests
                    tasks = []
                    for _ in range(self.concurrent_users // 3):  # Divide users among 3 scenarios
                        tasks.append(self._send_post_request(session, endpoint, payload))
                    
                    # Wait for the batch to complete
                    await asyncio.gather(*tasks)
                    
                    # Update progress
                    elapsed = min(time.time() - start_time, self.duration_seconds)
                    progress.update(task_id, completed=elapsed)
                    
                    # Rate limiting: 10 requests per second (per scenario)
                    await asyncio.sleep(0.1)

            async def _run_background_job_scenario(self, session, progress, task_id):
                start_time = time.time()
                payload = {
                    "script": "health-check-script.ps1",
                    "key": self.api_key
                }
                
                endpoint = f"{self.api_url}/jobs/v1/enqueue"
                
                while time.time() - start_time < self.duration_seconds:
                    # Create a batch of concurrent requests
                    tasks = []
                    for _ in range(self.concurrent_users // 3):  # Divide users among 3 scenarios
                        tasks.append(self._send_post_request(session, endpoint, payload, "Background Job"))
                    
                    # Wait for the batch to complete
                    await asyncio.gather(*tasks)
                    
                    # Update progress
                    elapsed = min(time.time() - start_time, self.duration_seconds)
                    progress.update(task_id, completed=elapsed)
                    
                    # Rate limiting: 15 requests per second (per scenario)
                    await asyncio.sleep(0.067)  # ~15 rps

            async def _send_get_request(self, session, url):
                start_time = time.time()
                try:
                    async with session.get(url, headers={"Accept": "application/json"}) as response:
                        await response.read()  # Read the full response
                        latency = time.time() - start_time
                        self.results["GET API"]["latencies"].append(latency)
                        
                        if response.status < 400:
                            self.results["GET API"]["success"] += 1
                        else:
                            self.results["GET API"]["fail"] += 1
                except Exception as e:
                    console.print(f"[red]GET request error: {e}[/red]")
                    self.results["GET API"]["fail"] += 1

            async def _send_post_request(self, session, url, payload, scenario="POST API"):
                start_time = time.time()
                try:
                    async with session.post(
                        url, 
                        json=payload,
                        headers={
                            "Content-Type": "application/json",
                            "Accept": "application/json"
                        }
                    ) as response:
                        await response.read()  # Read the full response
                        latency = time.time() - start_time
                        self.results[scenario]["latencies"].append(latency)
                        
                        if response.status < 400:
                            self.results[scenario]["success"] += 1
                        else:
                            self.results[scenario]["fail"] += 1
                except Exception as e:
                    console.print(f"[red]{scenario} request error: {e}[/red]")
                    self.results[scenario]["fail"] += 1

            def _generate_report(self):
                # Create reports directory
                os.makedirs("reports", exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Create console report
                table = Table(title="WebhookShell API Load Test Results")
                table.add_column("Scenario", style="cyan")
                table.add_column("Success", style="green")
                table.add_column("Fail", style="red")
                table.add_column("RPS", style="yellow")
                table.add_column("Min (ms)", style="blue")
                table.add_column("Max (ms)", style="blue")
                table.add_column("Avg (ms)", style="blue")
                table.add_column("p95 (ms)", style="magenta")
                
                for scenario, data in self.results.items():
                    if data["latencies"]:
                        latencies_ms = [l * 1000 for l in data["latencies"]]
                        rps = (data["success"] + data["fail"]) / self.duration_seconds
                        
                        min_latency = min(latencies_ms)
                        max_latency = max(latencies_ms)
                        avg_latency = statistics.mean(latencies_ms)
                        p95_latency = sorted(latencies_ms)[int(len(latencies_ms) * 0.95)]
                        
                        table.add_row(
                            scenario,
                            str(data["success"]),
                            str(data["fail"]),
                            f"{rps:.2f}",
                            f"{min_latency:.2f}",
                            f"{max_latency:.2f}",
                            f"{avg_latency:.2f}",
                            f"{p95_latency:.2f}"
                        )
                    else:
                        table.add_row(scenario, "0", "0", "0", "N/A", "N/A", "N/A", "N/A")
                
                console.print(table)
                
                # Save results to file
                report_file = f"reports/load_test_results_{timestamp}.json"
                with open(report_file, "w") as f:
                    json.dump({
                        "test_info": {
                            "api_url": self.api_url,
                            "concurrent_users": self.concurrent_users,
                            "duration_seconds": self.duration_seconds,
                            "timestamp": timestamp
                        },
                        "results": {
                            scenario: {
                                "success": data["success"],
                                "fail": data["fail"],
                                "latencies_stats": {
                                    "min_ms": min(data["latencies"]) * 1000 if data["latencies"] else 0,
                                    "max_ms": max(data["latencies"]) * 1000 if data["latencies"] else 0,
                                    "avg_ms": statistics.mean(data["latencies"]) * 1000 if data["latencies"] else 0,
                                    "p95_ms": sorted([l * 1000 for l in data["latencies"]])[int(len(data["latencies"]) * 0.95)] if data["latencies"] else 0
                                }
                            } for scenario, data in self.results.items()
                        }
                    }, f, indent=2)
                
                # Generate Markdown report summary for GitHub
                summary_file = "reports/summary-windows.md"
                with open(summary_file, "w") as f:
                    f.write("## Performance Test Summary\n\n")
                    f.write(f"- Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"- API URL: {self.api_url}\n")
                    f.write(f"- Concurrent Users: {self.concurrent_users}\n")
                    f.write(f"- Test Duration: {self.duration_seconds // 60} minutes\n\n")
                    
                    f.write("### Results Summary\n\n")
                    f.write("| Scenario | Success | Fail | RPS | Avg (ms) | p95 (ms) |\n")
                    f.write("|----------|---------|------|-----|----------|----------|\n")
                    
                    for scenario, data in self.results.items():
                        if data["latencies"]:
                            latencies_ms = [l * 1000 for l in data["latencies"]]
                            rps = (data["success"] + data["fail"]) / self.duration_seconds
                            avg_latency = statistics.mean(latencies_ms)
                            p95_latency = sorted(latencies_ms)[int(len(latencies_ms) * 0.95)]
                            
                            f.write(f"| {scenario}  |     {data['success']}    |     {data['fail']}     |     {rps:.2f}     |     {avg_latency:.2f}     |     {p95_latency:.2f}     |\n")
                        else:
                            f.write(f"| {scenario}  |     0     |     0     |     0     |     N/A     |     N/A    |\n")
                
                console.print(f"[bold green]Load test completed. Results saved to {report_file}[/bold green]")

        if __name__ == "__main__":
            main()
        EOL
            
    - name: Run Performance Tests
      run: |
            mkdir -p performance_tests/reports
            chmod +rwx performance_tests/reports
            chmod +rwx performance_tests/webhook_load_tester.py
            cd performance_tests
            python webhook_load_tester.py --api-url ${{ github.event.inputs.apiUrl || 'http://localhost:80' }} --concurrent-users ${{ github.event.inputs.concurrentUsers || '50' }} --duration ${{ github.event.inputs.testDuration || '5' }}
            
    - name: Upload Test Reports
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-reports
        path: performance_tests/reports/
        if-no-files-found: error  # Change to error to catch issues

    - name: Create PR Comment with Test Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        github-token: ${{secrets.GITHUB_TOKEN}}
        script: |
            const fs = require('fs');
            const summaryContent = fs.readFileSync('performance_tests/reports/summary-windows.md', 'utf8');
            
            github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summaryContent
            });
            
    - name: Display Test Summary for Windows Build
      run: |
            cat performance_tests/reports/summary-windows.md